name: "Zeiler_conv5"

input: "data"
input_dim: 128
input_dim: 3
input_dim: 64
input_dim: 64

input: "labels"
input_dim: 128 		# to be changed on-the-fly to match input dim
input_dim: 1            # 9(anchors)        
input_dim: 16  		# size for 224 input image, to be changed on-the-fly to match input dim
input_dim: 16		# size for 224 input image, to be changed on-the-fly to match input dim


layer {
	name: "conv1"
	type: "Convolution"
	bottom: "data"
	top: "conv1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 96
		kernel_size: 7
		pad: 3
		stride: 1
		weight_filler {
			type: "gaussian"
			std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
	name: "relu1"
	type: "ReLU"
	bottom: "conv1"
	top: "conv1"
}

layer {
	name: "norm1"
	type: "LRN"
	bottom: "conv1"
	top: "norm1"
	lrn_param {
		local_size: 3
		alpha: 0.00005
		beta: 0.75
		norm_region: WITHIN_CHANNEL
	}
}

layer {
	name: "pool1"
	type: "Pooling"
	bottom: "norm1"
	top: "pool1"
	pooling_param {
		kernel_size: 2
		stride: 2
		pad: 0
		pool: MAX
	}
}

layer {
	name: "conv2"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 256
		kernel_size: 5
		pad: 2
		stride: 1
		weight_filler {
			type: "gaussian"
			std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 1
		}
	}
}

layer {
	name: "relu2"
	type: "ReLU"
	bottom: "conv2"
	top: "conv2"
}

layer {
	name: "norm2"
	type: "LRN"
	bottom: "conv2"
	top: "norm2"
	lrn_param {
		local_size: 3
		alpha: 0.00005
		beta: 0.75
		norm_region: WITHIN_CHANNEL
	}
}

layer {
	name: "pool2"
	type: "Pooling"
	bottom: "norm2"
	top: "pool2"
	pooling_param {
		kernel_size: 2
		stride: 2
		pad: 0
		pool: MAX
	}
}

layer {
	name: "conv3"
	type: "Convolution"
	bottom: "pool2"
	top: "conv3"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 384
		kernel_size: 3
		pad: 1
		stride: 1
		weight_filler {
			type: "gaussian"
			std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
	name: "relu3"
	type: "ReLU"
	bottom: "conv3"
	top: "conv3"
}

layer {
	name: "conv4"
	type: "Convolution"
	bottom: "conv3"
	top: "conv4"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 384
		kernel_size: 3
		pad: 1
		stride: 1
		weight_filler {
			type: "gaussian"
			std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 1
		}
	}
}

layer {
	name: "relu4"
	type: "ReLU"
	bottom: "conv4"
	top: "conv4"
}

layer {
	name: "conv5"
	type: "Convolution"
	bottom: "conv4"
	top: "conv5"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		weight_filler {
			type: "gaussian"
			std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 1
		}
	}
}

layer {
	name: "relu5"
	type: "ReLU"
	bottom: "conv5"
	top: "conv5"
}




#layer {
#  name: "eltwise-sum"
#  type: "Eltwise"
#  bottom: "conv2_1"
#  bottom: "conv5"
#  top: "conv2_1_conv5_sum"
#  eltwise_param { operation: SUM }
#}

#layer {
#  name: "pool2_conv_proposal_concat"
#  bottom: "pool2"
#  bottom: "conv5"
#  top: "pool2_conv_proposal_concat"
#  type: "Concat"
#  concat_param {
#    axis: 1
#  }
#}

#------------------------output1----------------------------

layer {
   name: "conv_proposal1"
   type: "Convolution"
   bottom: "conv5"
   top: "conv_proposal1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "relu_proposal1"
   type: "ReLU"
   bottom: "conv_proposal1"
   top: "conv_proposal1"
}

layer {
   name: "proposal_cls_score1"
   type: "Convolution"
   bottom: "conv_proposal1"
   top: "proposal_cls_score1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output:  2   # 2(bg/fg) * 9(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}


# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score1"
   top: "proposal_cls_score_reshape"
   name: "proposal_cls_score_reshape"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels"
   top: "labels_reshape"
   name: "labels_reshape"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}


layer {
   name: "loss_cls1"
   type: "SoftmaxWithLoss"
   bottom: "proposal_cls_score_reshape"
   bottom: "labels_reshape"
   top: "loss_cls1"
   loss_weight: 1
}

layer {
   name: "accuarcy1"
   type: "Accuracy"
   bottom: "proposal_cls_score_reshape"
   bottom: "labels_reshape"
   top: "accuarcy1"
}



layer {
   name: "proposal_cls_prob1"
   type: "Softmax"
   bottom: "proposal_cls_score_reshape"
   top: "proposal_cls_prob1"

}

#--------------output2---------------------------------------

layer {
   name: "conv_proposal2"
   type: "Convolution"
   bottom: "conv3"
   top: "conv_proposal2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "relu_proposal2"
   type: "ReLU"
   bottom: "conv_proposal2"
   top: "conv_proposal2"
}

layer {
   name: "proposal_cls_score2"
   type: "Convolution"
   bottom: "conv_proposal2"
   top: "proposal_cls_score2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output:  2   # 2(bg/fg) * 9(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}


# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score2"
   top: "proposal_cls_score_reshape2"
   name: "proposal_cls_score_reshape2"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}


layer {
   name: "loss_cls2"
   type: "SoftmaxWithLoss"
   bottom: "proposal_cls_score_reshape2"
   bottom: "labels_reshape"
   top: "loss_cls2"
   loss_weight: 1
}

layer {
   name: "accuarcy2"
   type: "Accuracy"
   bottom: "proposal_cls_score_reshape2"
   bottom: "labels_reshape"
   top: "accuarcy2"
}



layer {
   name: "proposal_cls_prob2"
   type: "Softmax"
   bottom: "proposal_cls_score_reshape2"
   top: "proposal_cls_prob2"

}



layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "proposal_cls_prob1"
  bottom: "proposal_cls_prob2"
  top: "proposal_cls_prob"
  eltwise_param { operation: SUM }
}

layer {
  name: "proposal_cls_prob"
  bottom: "proposal_cls_prob"
  top: "proposal_cls_prob"
  type: "Power"
  power_param {
    power: 1
    scale: 0.5
    shift: 0
  }
}


layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "loss_cls1"
  bottom: "loss_cls2"
  top: "loss_cls"
  eltwise_param { operation: SUM }
}

layer {
  name: "loss_cls"
  bottom: "loss_cls"
  top: "loss_cls"
  type: "Power"
  power_param {
    power: 1
    scale: 0.5
    shift: 0
  }
}


layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "accuarcy1"
  bottom: "accuarcy2"
  top: "accuarcy"
  eltwise_param { operation: SUM }
}

layer {
  name: "accuarcy"
  bottom: "accuarcy"
  top: "accuarcy"
  type: "Power"
  power_param {
    power: 1
    scale: 0.5
    shift: 0
  }
}



layer {
   name: "silence"
   type: "Silence"
   bottom: "proposal_cls_prob"
}

